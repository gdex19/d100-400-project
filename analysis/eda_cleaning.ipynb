{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from final_project.data import read_data\n",
    "from final_project.preprocessing import (\n",
    "    compute_future_volatility, compute_returns, \n",
    "    compute_squared_returns, add_trading_days,\n",
    "    add_dates_and_times, add_features_responder,\n",
    "    write_data, winsorize_predictors,\n",
    "    add_lagged_metadata\n",
    ")\n",
    "from final_project.plotting import (\n",
    "    plot_hourly_averages, plot_day,\n",
    "    plot_mses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = read_data(\"btc\")\n",
    "display(df_btc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Compute Responder + Minimal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Next, we have to create the responder: 30 minute realized volatility for bitcoin. We also add some basic features: 1 minute, 5 minute, 30 minute, 60 minute, and 2 hour absolute and squared returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_btc = compute_returns(df_btc, [1, 5, 30, 60, 120])\n",
    "df_btc = compute_squared_returns(df_btc, [1, 5, 30, 60, 120])\n",
    "df_btc = add_lagged_metadata(df_btc)\n",
    "NUM_FEATURES = [f\"past_{x}m_{y}ret\" for y in [\"\", \"sq_\"] for x in [1, 5, 30, 60, 120]] + [\"past_30m_quote_volume\", \"past_30m_trades\"]\n",
    "\n",
    "\n",
    "df_btc = compute_future_volatility(df_btc)\n",
    "RESPONDER = \"future_30m_vol\"\n",
    "\n",
    "df_btc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Check distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc[NUM_FEATURES + [RESPONDER]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's look at histograms of 1m and 5m returns, 30m volatility, and lagged metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_btc, x=\"past_1m_ret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_btc, x=\"past_5m_ret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "It looks like returns should definitely be winsorized, very long tails!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_btc, x=\"future_30m_vol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "This also may benefit from being Winsorized, due to the fat right tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_btc, x=\"past_30m_trades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_btc, x=\"past_30m_quote_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Correlations + Relationships\n",
    "I expect large moves to be correlated with future volatility; large moves are likely followed by large moves, in absolute value. If markets are efficient, returns should not be autocorrelated, but squared returns may be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_btc[NUM_FEATURES + [RESPONDER]].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Future volatility seems to be correlated relatively more with longer previous squared returns and shorter absolute returns. This aligns with the hypothesis that the absolute size of the move is what matters.\n",
    "\n",
    "Let's look at autocorrelation plots next, for the responder, 1 minute returns, and 1 minute squared returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_acf(df_btc.iloc[::30][\"future_30m_vol\"].dropna(), lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "This looks reasonable: volatility is clustered over the span of hours, then the correlation decays, then it increases again\n",
    "close to 24 hours later! This strongly supports using time of day as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_acf(df_btc.iloc[::30][\"past_1m_ret\"].dropna(), lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Looks like the market is pretty efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_acf(df_btc.iloc[::30][\"past_1m_sq_ret\"].dropna(), lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We see a similar pattern to 30m vol here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Preliminary Hypotheses\n",
    "1. Time of day is an important feature, such as market open.\n",
    "    - This may break down on non-trading days.\n",
    "2. Events, such as tariff announcements or economic data release, will impact vol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = add_dates_and_times(df_btc)\n",
    "fig = plot_hourly_averages(df_btc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Looks like this should definitely be a feature. Volatility is highest at the beginning of the day when price discovery occurs,\n",
    "and decays throughout.\n",
    "\n",
    "Let's see if it's different if we group by NYSE trading/non-trading days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_btc = add_trading_days(df_btc)\n",
    "\n",
    "# Graph on trading days\n",
    "mask = df_btc[\"is_us_trading_day\"]\n",
    "fig = plot_hourly_averages(df_btc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph non-trading days\n",
    "fig = plot_hourly_averages(df_btc[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Looks like that will be a good responder to include!\n",
    "\n",
    "Now let's take a look at the announcement of tariffs and a rate cut. Trump's tariffs were announced on April 2, 2025, at around 4 PM ET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_day(df_btc, \"2025-04-02\", \"future_30m_vol\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "It looks like there was some impact; what about a fed rate announcement? We look at October 29, at 2 PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_day(df_btc, \"2025-10-29\", \"future_30m_vol\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Big spike!!! How about economic indicators? Now we look at the US CPI, released April 10 at 8:30 ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_day(df_btc, \"2025-04-10\", \"future_30m_vol\", 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "There is definitely a spike at 8:30, then another one at market open. So we should add features for economic data release and fed events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Single Predictor for Responder\n",
    "I have a hypothesis that some moving average of volatility will be the best predictor for forward looking vol - there is a tradeoff between incorporating new information and having a stable predictor. We will use a continuum of exponentially weighted moving averages of the mean of volatility, as well as a simple rolling mean, and see which have the lowest MSE if used as a baseline predictor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mses(df_btc, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Cool - we will add a feature for ewm with span 50 (close to 53!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Data Cleaning + Feature Addition\n",
    "Above we saw that we should add time of day, trading calendars, economics/fed data releases, and recent volatility as features. \n",
    "\n",
    "In fact, I will add trading days for UK, European, and Asian exchanges too. Also, I'll add daylight savings flags for each of these markets.\n",
    "\n",
    "We will also add all of the returns/squared returns data from before (slight risk of overfitting, can select features later). Finally, I'll add past 5 minute vol as a percentage of past 30m vol (increasing or decreasing??), and past 30m vol / past 60m vol.\n",
    "\n",
    "It also seems reasonable to add some longer realized vol measures, like 1 day, 1 week, and 1 month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Add features and responders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = read_data(\"btc\")\n",
    "df = add_features_responder(df_raw)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Clip data\n",
    "Here, I will clip my numerical features to avoid having super high-leverage points on outliers. The data is quite clean in general, though.\n",
    "\n",
    "I will use percentiles from the first 6 months of 2024 to avoid lookahead when we change to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unclipped data to parquet\n",
    "write_data(df, \"clean_data\")\n",
    "\n",
    "df_clipped = winsorize_predictors(df)\n",
    "write_data(df_clipped, \"clean_data_clipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "The prepare_data script can also do the steps above!\n",
    "\n",
    "Let's take a quick look at correlation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_project.preprocessing import NUM_FEATURES\n",
    "sns.heatmap(df[NUM_FEATURES].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
