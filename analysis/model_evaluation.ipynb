{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from final_project.modeling import load_models, EVENT_WEIGHT\n",
    "from final_project.data import read_data, split_data\n",
    "from final_project.preprocessing import NUM_FEATURES, CAT_FEATURES, RESPONDER\n",
    "from final_project.evaluation import evaluate_predictions, get_pred_summary, get_models_and_val_data\n",
    "from final_project.plotting import plot_pred_vs_true, plot_day_predictions, plot_feature_relevance, plot_pdps\n",
    "\n",
    "# Silence feature name warnings \n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"X does not have valid feature names\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_raw, lgbm_raw, X_val_raw, y_val_raw = get_models_and_val_data(\"clean_data\")\n",
    "glm_clip, lgbm_clip, X_val_clip, y_val_clip = get_models_and_val_data(\"clean_data_clipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_raw = get_pred_summary(glm_raw, lgbm_raw, X_val_raw, y_val_raw)\n",
    "df_pred_clip = get_pred_summary(glm_clip, lgbm_clip, X_val_clip, y_val_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Evaluate Models\n",
    "Added baseline (past_50m_span_ewm_vol) to compare to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate glm\n",
    "glm_eval_raw = evaluate_predictions(\n",
    "    df_pred_raw[\"y_true\"], df_pred_raw[\"glm_y_pred\"], \n",
    "    df_pred_raw[\"weight\"]\n",
    ")\n",
    "print(\"Unclipped:\\n\")\n",
    "print(glm_eval_raw)\n",
    "glm_eval_clip = evaluate_predictions(\n",
    "    df_pred_clip[\"y_true\"], df_pred_clip[\"glm_y_pred\"], \n",
    "    df_pred_clip[\"weight\"]\n",
    ")\n",
    "print(\"\\nClipped:\\n\")\n",
    "print(glm_eval_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate lgbm\n",
    "lgbm_eval_raw = evaluate_predictions(\n",
    "    df_pred_raw[\"y_true\"], df_pred_raw[\"lgbm_y_pred\"], \n",
    "    df_pred_raw[\"weight\"]\n",
    ")\n",
    "print(\"Unclipped:\\n\")\n",
    "print(glm_eval_raw)\n",
    "lgbm_eval_clip = evaluate_predictions(\n",
    "    df_pred_clip[\"y_true\"], df_pred_clip[\"lgbm_y_pred\"], \n",
    "    df_pred_clip[\"weight\"]\n",
    ")\n",
    "print(\"\\nClipped:\\n\")\n",
    "print(lgbm_eval_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline\n",
    "base_eval_raw = evaluate_predictions(\n",
    "    df_pred_raw[\"y_true\"], df_pred_raw[\"baseline_y_pred\"], \n",
    "    df_pred_raw[\"weight\"]\n",
    ")\n",
    "print(\"Unclipped:\\n\")\n",
    "print(base_eval_raw)\n",
    "base_eval_clip = evaluate_predictions(\n",
    "    df_pred_clip[\"y_true\"], df_pred_clip[\"baseline_y_pred\"], \n",
    "    df_pred_clip[\"weight\"]\n",
    ")\n",
    "print(\"\\nClipped:\\n\")\n",
    "print(base_eval_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Looks like the GBT outperformed the GLM on all measures! And clipped data is significantly better than unclipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Predicted vs. Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pred vs. true for glm\n",
    "fig = plot_pred_vs_true(df_pred_clip, \"glm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pred vs. true for lgbm\n",
    "fig = plot_pred_vs_true(df_pred_clip, \"lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "This should look better with logs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pred vs. true for glm, log axes\n",
    "fig = plot_pred_vs_true(df_pred_clip, \"glm\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pred vs. true for lgbm, log axes\n",
    "fig = plot_pred_vs_true(df_pred_clip, \"lgbm\", log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pred vs. true for baseline, log axes\n",
    "fig = plot_pred_vs_true(df_pred_clip, \"baseline\", log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Look at predicted value throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_day_predictions(df_pred_clip, \"2025-10-30\")\n",
    "fig = plot_day_predictions(df_pred_clip, \"2025-07-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Feature Relevance and PDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot glm features\n",
    "glm_top_5 = plot_feature_relevance(glm_clip, X_val_clip, y_val_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lgbm features\n",
    "lgbm_top_5 = plot_feature_relevance(lgbm_clip, X_val_clip, y_val_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdps(glm_clip, X_val_clip, y_val_clip, n_top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdps(lgbm_clip, X_val_clip, y_val_clip, n_top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
